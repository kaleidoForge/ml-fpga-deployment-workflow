{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gamma/neutron discrimination based on ML\n",
    "\n",
    "## Machine learning - Training and compression\n",
    "\n",
    "--- \n",
    "\n",
    "- Workflow based on R. S. Molina, I. R. Morales, M. L. Crespo, V. G. Costa, S. Carrato and G. Ramponi, \"An End-to-End Workflow to Efficiently Compress and Deploy DNN Classifiers on SoC/FPGA\", in IEEE Embedded Systems Letters, vol. 16, no. 3, pp. 255-258, Sept. 2024, doi: 10.1109/LES.2023.3343030.\n",
    "\n",
    "- Code adapted from the official repository of \"An End-to-End Workflow to Efficiently Compress and Deploy DNN Classifiers on SoC/FPGA\"\n",
    "\n",
    "- Using open dataset from: https://doi.org/10.5281/zenodo.8037058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "## Tensorflow + Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "## Pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "## Quantization\n",
    "from qkeras import *\n",
    "\n",
    "## Knowledge Distillation\n",
    "from distillationClassKeras import *\n",
    "\n",
    "## Metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "## Pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "## Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import hls4ml\n",
    "\n",
    "import plotting\n",
    "\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the training and testing datasets\n",
    "\n",
    "def preproc_dataset_(signal_dfN, signal_dfG):\n",
    "    \n",
    "    # Label in csv file corresponds to the signal class\n",
    "    _LABEL_COLUMN = 'class'\n",
    "    \n",
    "    dfTest = pd.DataFrame()\n",
    "    dfTrain = pd.DataFrame()\n",
    "    \n",
    "    signal_dfN = shuffle(signal_dfN)\n",
    "    signal_dfG = shuffle(signal_dfG)\n",
    "    \n",
    "    signal_df = pd.concat([signal_dfN, signal_dfG])\n",
    "    \n",
    "    for k in range(0,2):\n",
    "     \n",
    "        df2 = signal_df[signal_df[_LABEL_COLUMN].isin([k])]\n",
    "        \n",
    "        df_tr = df2[:10000]\n",
    "        df_t = df2[10001:10900]\n",
    "        \n",
    "        dfTrain = pd.concat([df_tr, dfTrain])   \n",
    "        dfTest = pd.concat([df_t, dfTest])\n",
    "        \n",
    "    \n",
    "    return dfTrain, dfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the dataset for training\n",
    "\n",
    "PATH = '../dataset/'\n",
    "\n",
    "GAMMA_DATASET_FILE = PATH + 'gamma_label.csv'\n",
    "NEUTRON_DATASET_FILE = PATH + 'neutron_label.csv'\n",
    "\n",
    "TEST_DATASET_FILE = PATH + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets in Pandas data frame \n",
    "\n",
    "dfGamma = pd.read_csv(GAMMA_DATASET_FILE)\n",
    "dfNeutron = pd.read_csv(NEUTRON_DATASET_FILE)\n",
    "dfTestGN = pd.read_csv(TEST_DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing dataset for training \n",
    "df_train, dfTest = preproc_dataset_(dfNeutron, dfGamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ = df_train.pop('class')\n",
    "dfTest_ = dfTest.pop('class')\n",
    "\n",
    "# One-hot encoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_train_)\n",
    "y = to_categorical(df_train_, 2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "yTest = le.fit_transform(dfTest_)\n",
    "yTest = to_categorical(dfTest_, 2)\n",
    "\n",
    "# Split training dataset into training and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(df_train, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine leraning - Training and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for the teacher architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the teacher model\n",
    "lr = 0.001\n",
    "neurons_teacher = [10, 5, 7, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teacher architecture is shown in the next figure. It is composed of five hidden layers, an input of 161 elements, and an output of two elements that correspond to gamma and neutron classification.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src = \"img/teacherArchitecture.png\" width = 50%>\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Teacher architecture.</figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines the teacher architecture composed of six dense layers.\n",
    "# bestHP contains the number of neurons per each dense layer.\n",
    "\n",
    "\n",
    "def teacher_topology(bestHP):\n",
    "\n",
    "    teacher = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(161, )),\n",
    "             \n",
    "            Dense(bestHP[0], name='fc1', input_shape=(161,), kernel_regularizer=l2(0.001),),\n",
    "            Activation(activation='relu', name='relu1'),       \n",
    "\n",
    "            Dense(bestHP[1], name='fc2', kernel_regularizer=l2(0.001)),\n",
    "            Activation(activation='relu', name='relu2'),\n",
    "        \n",
    "            Dense(bestHP[2], name='fc3', kernel_regularizer=l2(0.001)),\n",
    "            Activation(activation='relu', name='relu3'),      \n",
    "            Dropout(0.1),    \n",
    "            \n",
    "            Dense(bestHP[3], name='fc4', kernel_regularizer=l2(0.001)),\n",
    "            Activation(activation='relu', name='relu4'), \n",
    "            Dropout(0.2), \n",
    "\n",
    "            Dense(bestHP[4], name='fc5'),\n",
    "            Activation(activation='relu', name='relu5'), \n",
    "            Dropout(0.2),            \n",
    "\n",
    "            Dense(2, name='output'),\n",
    "            Activation(activation='sigmoid', name='activationOutput'),\n",
    "            \n",
    "        ],\n",
    "        name=\"teacher_MLP\",\n",
    "    )\n",
    "\n",
    "    teacher.summary()\n",
    "\n",
    "\n",
    "    return teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_teacher(neurons_teacher):\n",
    "\n",
    "    model = teacher_topology(neurons_teacher)\n",
    "       \n",
    "    opt = Adam(lr)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=['binary_crossentropy'], metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teacher is defined by means of the `build_teacher()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = build_teacher(neurons_teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the teacher model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This steps performs the neural network training through `teacher_model.fit()`. \n",
    "\n",
    "- _x_ and _y_ are the defined as _x_train_ and _y_train_, respectively. The validation data corresponds to _x_val_ and _y_val_. \n",
    "\n",
    "- The batch size is defined as 64, while the number of epochs is 32. \n",
    "\n",
    "- Callbacks are used to early stop the training process if no improvement in loss is obtained during training (_EarlyStopping_), and the reduction of the learning rate based on the accuracy metric (_ReduceLROnPlateau_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.4, patience=3, verbose=1)\n",
    "            ] \n",
    "\n",
    "history_teacher  = teacher_model.fit(x=x_train, y=y_train,\n",
    "                  validation_data=(x_val, y_val), \n",
    "                  batch_size = 64,\n",
    "                  epochs=32,\n",
    "                  callbacks = [callbacks],\n",
    "                  verbose=1\n",
    "                  )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot accuracy and loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots of the behavior of accuracy and loss during training are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_teacher.history.keys())\n",
    "\n",
    "## Plot for accuracy\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(history_teacher.history['accuracy'])\n",
    "plt.plot(history_teacher.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "## Plot for loss\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(history_teacher.history['loss'])\n",
    "plt.plot(history_teacher.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, a pre-trained model can be loaded to skip the training step. To enable this, uncomment the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_model = load_model('models/teacherModel_GN_smr4078.h5')\n",
    "# teacher_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the confusion matrix using the testing dataset \n",
    "y_pred_probs = teacher_model.predict(dfTest)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, you need to convert it back to class indices\n",
    "y_true = np.argmax(yTest, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix - Teacher model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight distribution\n",
    "\n",
    "weights = np.concatenate([w.flatten() for w in teacher_model.get_weights()])\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.hist(weights, bins=60, color='green', alpha=0.6)\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Model MLP for G/N discrimination - Weight Distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, save the teacher model [uncomment the following line]\n",
    "# teacher_model.save('models/teacherModel_GN_smr4078.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the reduced model that will be deployed onto the FPGA (hereinafter referred to as the student network), a **KD** learning approach will be employed. This method involves the teacher model transferring its knowledge to the student model. The latter is defined using quantization and pruning strategies.  \n",
    "\n",
    "In this project, the number of bits was set to eight, and the target sparsity was set to 50%. \n",
    "\n",
    "**Qkeras** is employed to define the student model in a quantized manner. \n",
    "\n",
    "**For more information regarding QKeras:** Coelho, C. N., Kuusela, A., Zhuang, H., Aarrestad, T., Loncar, V., Ngadiuba, J., ... & Summers, S. (2020). _Ultra low-latency, low-area inference accelerators using heterogeneous deep quantization with QKeras and hls4ml_. arXiv preprint arXiv:2006.10159, 108."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for the student model\n",
    "lr = 0.001\n",
    "neurons_student = [6, 4, 2, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define student architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src = \"img/studentArchitecture.png\" width = 50%>\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Student architecture.</figcaption>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_architecture(neurons_student):\n",
    "    \n",
    "    '''\n",
    "    Model to be trained. Defined with quantization strategies. \n",
    "    Input: hyperparams. (bestHP)\n",
    "    Output: compressed model (studentQ_MLP). \n",
    "\n",
    "    '''\n",
    "    ######## ---------------------------  Model definition - 1D STUDENT -----------------------------------------\n",
    "\n",
    "    # Number of bits \n",
    "    ## 8-bits\n",
    "    kernelQ = \"quantized_bits(8,4)\"\n",
    "    biasQ = \"quantized_bits(8,4)\"\n",
    "    activationQ = 'quantized_bits(8)'\n",
    "    \n",
    "    ## 16-bits\n",
    "    kernelQ_16b = \"quantized_bits(16,6)\"\n",
    "    biasQ_16b = \"quantized_bits(16,6)\"\n",
    "    activationQ_16b = 'quantized_bits(16)'\n",
    "    \n",
    "    studentQ_MLP = keras.Sequential(\n",
    "            [   \n",
    "                Input(shape=(161,), name='inputLayer'),\n",
    "                QDense(neurons_student[0], name='fc1',\n",
    "                        kernel_quantizer= kernelQ, bias_quantizer= biasQ,\n",
    "                        kernel_initializer='lecun_uniform'),\n",
    "                QActivation(activation= activationQ ,  name='relu0'),\n",
    "                \n",
    "                Dropout(0.1),\n",
    "                    \n",
    "                QDense(neurons_student[1], name='fc2',\n",
    "                        kernel_quantizer=kernelQ, bias_quantizer=biasQ,\n",
    "                        kernel_initializer='lecun_uniform'),\n",
    "                QActivation(activation=activationQ, name='relu1'), \n",
    "                Dropout(0.1),\n",
    "                \n",
    "\n",
    "                QDense(neurons_student[2], name='fc3',\n",
    "                        kernel_quantizer=kernelQ, bias_quantizer=biasQ,\n",
    "                        kernel_initializer='lecun_uniform'),\n",
    "                QActivation(activation=activationQ, name='relu2'), \n",
    "                Dropout(0.1),\n",
    "\n",
    "                QDense(neurons_student[3], name='fc4',\n",
    "                        kernel_quantizer=kernelQ, bias_quantizer=biasQ,\n",
    "                        kernel_initializer='lecun_uniform'),\n",
    "                QActivation(activation=activationQ, name='relu3'), \n",
    "                Dropout(0.2),\n",
    "\n",
    "                QDense(neurons_student[4], name='fc5',\n",
    "                       kernel_quantizer=kernelQ, bias_quantizer=biasQ,\n",
    "                       kernel_initializer='lecun_uniform'),\n",
    "                QActivation(activation=activationQ, name='relu4'), \n",
    "                Dropout(0.2),\n",
    "               \n",
    "                QDense(2, name='output',\n",
    "                        kernel_quantizer= kernelQ_16b, bias_quantizer= biasQ_16b,\n",
    "                        kernel_initializer='lecun_uniform'),\n",
    "                Activation(activation='sigmoid', name='outputActivation')\n",
    "\n",
    "                \n",
    "            ],\n",
    "            name=\"studentMLP\",\n",
    "        )\n",
    "\n",
    "\n",
    "    return studentQ_MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build student model with pruning strategy\n",
    "\n",
    "def build_student(student_neurons):\n",
    "\n",
    "    '''\n",
    "    Model to be compressed. Defined with quantization strategies. \n",
    "    Input: hyperparams (student_neurons).\n",
    "    Output: compressed model (studentQ). \n",
    "\n",
    "    '''\n",
    "    \n",
    "    qmodel = student_architecture(student_neurons)\n",
    "\n",
    "    # Pruning parameters \n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0, final_sparsity=0.5, begin_step=0, end_step=1000\n",
    "    )\n",
    "    }\n",
    "    \n",
    "    studentQ = prune.prune_low_magnitude(qmodel, **pruning_params)\n",
    "    \n",
    "    \n",
    "    return studentQ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile student model for KD + QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentQ = build_student(neurons_student)\n",
    "\n",
    "distilled_student = Distiller(student=studentQ, teacher=teacher_model)\n",
    "\n",
    "adam = Adam(lr)\n",
    "\n",
    "train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "distilled_student.compile(\n",
    "        optimizer=adam,\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "        alpha=0.3, \n",
    "        temperature=9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "                tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(monitor='sparse_categorical_accuracy', factor=0.3, patience=3, verbose=1),\n",
    "                ]  \n",
    "\n",
    "\n",
    "callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "\n",
    "history_studentQPKD = distilled_student.fit(x_train, train_labels, \n",
    "                               batch_size = 64, \n",
    "                               epochs= 32, \n",
    "                               validation_split=0.2,\n",
    "                               callbacks = callbacks\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(history_studentQPKD.history['sparse_categorical_accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_studentQPKD.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(history_studentQPKD.history['student_loss'], label='Train Loss')\n",
    "plt.plot(history_studentQPKD.history['val_student_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight distribution after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = np.concatenate([w.flatten() for w in distilled_student.student.get_weights()])\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.hist(weights, bins=60, color='green', alpha=0.6)\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Model for G/N - Weight Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix to observe the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Obtain the confusion matrix using the testing dataset \n",
    "y_pred_probs = distilled_student.student.predict(dfTest)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, you need to convert it back to class indices\n",
    "y_true = np.argmax(yTest, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix - Student model Q, P, and KD')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model for single inputs. \n",
    "\n",
    "The variable **indexPrediction** corresponds to the signals' index  in the testing dataset. You can change this number to observe how the ML-based model predicts for specific inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the signal in the testing dataset\n",
    "indexPrediction = 1500\n",
    "\n",
    "x_input = dfTest.iloc[indexPrediction]\n",
    "y_label = yTest[indexPrediction]\n",
    "\n",
    "inputPred = array([x_input])\n",
    "\n",
    "y_pred = distilled_student.student.predict(inputPred) \n",
    "print(\"> Input %s -> Predicted = %s | \" % (y_label, y_pred))\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.xlabel('Samples', fontsize=11)\n",
    "plt.ylabel('Amplitude', fontsize=11)\n",
    "plt.grid(True, alpha=1.0)\n",
    "plt.plot(x_input.values,  label=\"Signal 1\", color='navy', markersize=7, lw=1)\n",
    "\n",
    "plt.title('Signal used for inference - Pulse: %s, Label: %s, Prediction: %s' % (indexPrediction, y_label, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = strip_pruning(distilled_student.student)\n",
    "model.summary()\n",
    "\n",
    "model.save('../models/studentModel_GN_smr4110.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with a hardware synthesis tool for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hls4ml\n",
    "\n",
    "**hls4ml** is a Python package developed for converting machine learning (ML) models into HLS (High-Level Synthesis) projects, enabling deployment of ML-based inference on hardware like FPGAs. More details can be found at [hls4ml documentation](https://fastmachinelearning.org/hls4ml/).\n",
    "\n",
    "The user can control several options related to the model, including:\n",
    "\n",
    "- **Precision:** Define the precision of the calculations in your model (e.g., fixed-point or floating-point representation).\n",
    "\n",
    "- **Dataflow/Resource Reuse:** Control the level of parallelism or streaming in model implementations, with varying degrees of pipelining.\n",
    "\n",
    "\n",
    "\n",
    "hls4ml has compatibility with **Quantization Aware Training:** Achieve optimized performance at low precision by using tools like QKeras. Models trained with QKeras benefit automatically from hls4ml's parsing of QKeras models during inference.\n",
    "\n",
    "An HLS configuration should be created using the function `hls4ml.utils.config_from_keras_model(kerasModel, granularity)`, where kerasModel is the pre-trained model you want to implement on an FPGA, and granularity determines the configuration level. The two possible values for granularity are:\n",
    "\n",
    "- 'model': The same configuration applies to the entire model (e.g., all layers use 16-bit fixed-point precision).\n",
    "\n",
    "- 'name': Layer-specific configurations can be applied (e.g., the input layer can be defined in 8-bit fixed-point precision, while the second layer is set to 16-bit fixed-point precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH Xilinx Virtual Machine\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis_HLS/2022.2/bin:' + os.environ['PATH']\n",
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(hls_config)\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Latency'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<8,4>'\n",
    "\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,10>'\n",
    "hls_config['LayerName']['outputActivation']['Strategy'] = 'Stable'\n",
    "hls_config['LayerName']['inputLayer']['Precision'] = 'ap_fixed<16, 6>'   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for Vitis HLS as backend.\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "# HLSConfig correspond to the configuration created in hls_config \n",
    "cfg['HLSConfig']  = hls_config\n",
    "# Model to be converted\n",
    "cfg['KerasModel'] = model\n",
    "# Folder where the HLS project will be created\n",
    "cfg['OutputDir']  = 'hlsPrj/'\n",
    "# FPGA part \n",
    "cfg['Part'] = 'xc7z020clg484-1'  \n",
    "  \n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will perform the synthesis of the HLS project, showing the main results (latency and resource usage) once the process is completed. \n",
    "\n",
    "hls_model.build(csim=False, export=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once this stage is complete, you can return to the wiki to continue with the next steps.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

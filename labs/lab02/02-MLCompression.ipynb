{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](../img/general/header_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compression \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "## Tensorflow + Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "## Quantization\n",
    "from qkeras import *\n",
    "\n",
    "## Datasets\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "from distillationClassKeras import *\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously trained MNIST fully connected (FC) model\n",
    "model = load_model('../models/mnistModel_FC.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the model architecture and parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "For this laboratory, we will work with the MNIST dataset of handwritten digits, which is commonly used for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset, split into training and testing sets (data and labels)\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train_norm = x_train / 255.0\n",
    "x_test_norm = x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "n_classes = 10\n",
    "\n",
    "# One-hot encode the class labels\n",
    "y_train = to_categorical(y_train, num_classes=n_classes)\n",
    "y_test = to_categorical(y_test, num_classes=n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "Pruning is a technique used to reduce the size and complexity of a deep learning model by removing unnecessary weights or neurons. Its main goal is to improve model efficiency by reducing memory usage and accelerating inference, while maintaining performance as much as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "\n",
    "epochs = 16\n",
    "batch = 64\n",
    "val_split = 0.2\n",
    "\n",
    "# Number of training samples after validation split\n",
    "n_train_samples = int(x_train.shape[0] * (1 - val_split))\n",
    "\n",
    "# Steps per epoch\n",
    "steps_per_epoch = math.ceil(n_train_samples / batch)\n",
    "\n",
    "# Total number of pruning steps\n",
    "# If end_step is too small, pruning happens too aggressively and may degrade accuracy. If it is too large, the model may not reach the target sparsity.\n",
    "end_step = epochs * steps_per_epoch\n",
    "\n",
    "final_sparsity = 0.3\n",
    "\n",
    "# begin_step = 0\n",
    "# → Pruning starts from the beginning of training\n",
    "\n",
    "# end_step = epochs × steps_per_epoch\n",
    "# → The model reaches the target sparsity at the end of training\n",
    "\n",
    "# final_sparsity = 0.3\n",
    "# → 30% of the model weights will be zero\n",
    "\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=final_sparsity,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply magnitude-based pruning to the original model\n",
    "modelP = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Training configuration\n",
    "lr = 0.001\n",
    "loss = 'categorical_crossentropy'\n",
    "op = Adam(learning_rate=lr)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "# Compile the pruned model\n",
    "modelP.compile(\n",
    "    optimizer=op,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pruned model\n",
    "historyP = modelP.fit(\n",
    "    x_train_norm,\n",
    "    y_train,\n",
    "    validation_split=val_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()],  # Required to update pruning step during training\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs for the pruned model\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.plot(historyP.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(historyP.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the test set using the pruned model\n",
    "y_pred_probs = modelP.predict(x_test_norm)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, convert it back to class indices\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Purples\")\n",
    "\n",
    "plt.title('Confusion Matrix for MNIST – Pruned Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Model\n",
    "\n",
    "Once the model has been trained and evaluated, it can be saved to disk for later use. Saving the model allows it to be reloaded without retraining, preserving both the learned weights and the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pruning wrappers to obtain the final optimized model\n",
    "modelP = strip_pruning(modelP)\n",
    "\n",
    "# Save the trained model to disk\n",
    "modelP.save('models/mnistModel_FC_P.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise:\n",
    "\n",
    "Modify the value of final_sparsity (0.1, 0.3, 0.5, 0.7, 0.9) and report the evaluation metrics for each pruned model.\n",
    "\n",
    "- What conclusions can you draw regarding the relationship between sparsity level, model performance, and efficiency?\n",
    "\n",
    "\n",
    "#### Pruning Results Summary\n",
    "\n",
    "| Experiment | Final Sparsity | Train Accuracy | Validation Accuracy | Test Accuracy | Observations |\n",
    "|-----------|----------------|----------------|---------------------|---------------|--------------|\n",
    "| 1 | 0.1 | | | | |\n",
    "| 2 | 0.3 | | | | |\n",
    "| 3 | 0.5 | | | | |\n",
    "| 4 | 0.7 | | | | |\n",
    "| 5 | 0.9 | | | | |\n",
    "\n",
    "\n",
    "**Guiding questions:**\n",
    "- At what sparsity level does accuracy start to degrade significantly?\n",
    "- Is there a sparsity range where efficiency improves with minimal loss in performance?\n",
    "- How does aggressive pruning affect training stability and convergence?\n",
    "\n",
    "\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization \n",
    "Quantization is a technique that reduces the numerical precision of a neural network’s parameters by converting floating-point values (e.g., 32-bit) into lower-precision representations, such as 16-bit or even 8-bit. The main goal is to reduce model size and accelerate inference, especially on resource-constrained devices such as mobile phones or microcontrollers.\n",
    "\n",
    "In this laboratory, we will use Quantization-Aware Training (QAT) and Quantization-Aware Pruning (QAP).\n",
    "\n",
    "- QAT is a training technique in which the model learns to adapt to quantization before being deployed on hardware. Instead of training the model at full precision (32-bit floating point) and quantizing it afterward, quantization effects are simulated during training.\n",
    "\n",
    "- QAP combines pruning (removal of unnecessary connections in the neural network) with quantization-aware training. The goal is to reduce the model size before quantization, resulting in a more efficient network without significantly sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization-aware training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is redefined using QKeras, an extension of Keras designed to create and train quantized neural network models. Its main objective is to optimize models for hardware platforms with limited resources, such as FPGAs, microcontrollers, and embedded accelerators.\n",
    "\n",
    "- It allows defining weights and activations with different precision levels (e.g., 8-bit, 4-bit, ternary -1,0,1, etc.).\n",
    "\n",
    "- By reducing numerical precision, it decreases memory usage and computational cost.\n",
    "\n",
    "- It facilitates the conversion of quantized models into efficient FPGA implementations, ensuring compatibility with hls4ml.\n",
    "\n",
    "- It is compatible with standard Keras layers, while providing additional support for low-precision configurations.\n",
    "\n",
    "Once the model has been redefined and trained using QKeras, the training process is carried out as usual.\n",
    "\n",
    "**QKeras reference:** \n",
    "\n",
    "Coelho, C. N., Kuusela, A., Zhuang, H., Aarrestad, T., Loncar, V., Ngadiuba, J., ... & Summers, S. (2020). _Ultra low-latency, low-area inference accelerators using heterogeneous deep quantization with QKeras and hls4ml_. arXiv preprint arXiv:2006.10159, 108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the number of bits for kernel, bias, and activations\n",
    "# 8-bit quantization\n",
    "\n",
    "kernelQ = \"quantized_bits(8, 4, alpha=1)\"\n",
    "biasQ = \"quantized_bits(8, 4, alpha=1)\"\n",
    "activationQ = \"quantized_bits(8, 4)\"\n",
    "\n",
    "# Definition of a Quantization-Aware Training (QAT) model using QKeras\n",
    "modelQAT = Sequential(\n",
    "    [\n",
    "        # Flatten the 2D input image into a 1D vector\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "\n",
    "        # First quantized fully connected layer\n",
    "        QDense(\n",
    "            100,\n",
    "            name=\"fc1\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu1\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Second quantized fully connected layer\n",
    "        QDense(\n",
    "            50,\n",
    "            name=\"fc2\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu2\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Third quantized fully connected layer\n",
    "        QDense(\n",
    "            20,\n",
    "            name=\"fc3\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu3\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Output quantized layer\n",
    "        QDense(\n",
    "            10,\n",
    "            name=\"output\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "\n",
    "        # Softmax activation kept in full precision\n",
    "        Activation(activation=\"softmax\", name=\"softmax\"),\n",
    "    ],\n",
    "    name=\"quantizedModel\",\n",
    ")\n",
    "\n",
    "\n",
    "# In QKeras-based QAT, weights and activations are quantized during training, allowing the model to learn robustness to reduced numerical precision.\n",
    "# The final softmax layer is typically kept in full precision to preserve numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelQAT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 16\n",
    "lr = 0.001\n",
    "loss = 'categorical_crossentropy'\n",
    "op = Adam(learning_rate=lr)\n",
    "metrics = ['accuracy']\n",
    "batch = 64\n",
    "val_split = 0.2\n",
    "\n",
    "# Compile the QAT model\n",
    "modelQAT.compile(\n",
    "    optimizer=op,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Train the QAT model\n",
    "historyQAT = modelQAT.fit(\n",
    "    x_train_norm,\n",
    "    y_train,\n",
    "    validation_split=val_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(historyQAT.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(historyQAT.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the test set using the QAT model\n",
    "y_pred_probs = modelQAT.predict(x_test_norm)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, convert it back to class indices\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Purples\")\n",
    "\n",
    "plt.title('Confusion Matrix for MNIST – QAT Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization-aware pruning\n",
    "\n",
    "Quantization-aware pruning (QAP) combines pruning with quantization-aware training. The goal is to reduce the model size after quantization, resulting in a more efficient neural network without significantly sacrificing accuracy.\n",
    "\n",
    "In QAP, pruning and quantization effects are applied simultaneously during training. An incorrect pruning schedule may lead to excessive accuracy degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "epochs = 16\n",
    "batch = 64\n",
    "val_split = 0.2\n",
    "\n",
    "# Number of training samples after validation split\n",
    "n_train_samples = int(x_train_norm.shape[0] * (1 - val_split))\n",
    "\n",
    "# Steps per epoch\n",
    "steps_per_epoch = math.ceil(n_train_samples / batch)\n",
    "\n",
    "# Total number of pruning steps\n",
    "end_step = epochs * steps_per_epoch\n",
    "\n",
    "# begin_step = 0\n",
    "# → Pruning starts from the beginning of training\n",
    "\n",
    "# end_step = epochs × steps_per_epoch\n",
    "# → The model reaches the target sparsity at the end of training\n",
    "\n",
    "# final_sparsity = 0.3\n",
    "# → 30% of the weights will be pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning strategy\n",
    "final_sparsity = 0.3\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=final_sparsity,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization strategy (for QAP)\n",
    "\n",
    "# Definition of the number of bits for kernel, bias, and activations\n",
    "# 8-bit quantization\n",
    "kernelQ = \"quantized_bits(8, 4, alpha=1)\"\n",
    "biasQ = \"quantized_bits(8, 4, alpha=1)\"\n",
    "activationQ = \"quantized_bits(8, 4)\"\n",
    "\n",
    "# Definition of a quantized model (QKeras) to be used for Quantization-Aware Pruning (QAP)\n",
    "modelQ_QAP = Sequential(\n",
    "    [\n",
    "        # Flatten the 2D input image into a 1D vector\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "\n",
    "        # First quantized fully connected layer\n",
    "        QDense(\n",
    "            100,\n",
    "            name=\"fc1\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu1\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Second quantized fully connected layer\n",
    "        QDense(\n",
    "            50,\n",
    "            name=\"fc2\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu2\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Third quantized fully connected layer\n",
    "        QDense(\n",
    "            20,\n",
    "            name=\"fc3\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "        QActivation(activation=activationQ, name=\"relu3\"),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        # Output quantized layer\n",
    "        QDense(\n",
    "            10,\n",
    "            name=\"output\",\n",
    "            kernel_quantizer=kernelQ,\n",
    "            bias_quantizer=biasQ,\n",
    "            kernel_initializer=\"lecun_uniform\",\n",
    "        ),\n",
    "\n",
    "        # Softmax activation kept in full precision for numerical stability\n",
    "        Activation(activation=\"softmax\", name=\"softmax\"),\n",
    "    ],\n",
    "    name=\"quantizedModel\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "epochs = 16\n",
    "lr = 0.001\n",
    "loss = 'categorical_crossentropy'\n",
    "op = Adam(learning_rate=lr)\n",
    "metrics = ['accuracy']\n",
    "batch = 64\n",
    "val_split = 0.2\n",
    "\n",
    "# Apply magnitude-based pruning to the quantized model (QAP)\n",
    "modelQAP = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "    modelQ_QAP, **pruning_params\n",
    ")\n",
    "\n",
    "# Compile the QAP model\n",
    "modelQAP.compile(\n",
    "    optimizer=op,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Train the QAP model\n",
    "history_QAP = modelQAP.fit(\n",
    "    x_train_norm,\n",
    "    y_train,\n",
    "    validation_split=val_split,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch,\n",
    "    callbacks=[tfmot.sparsity.keras.UpdatePruningStep()],  # Required for pruning schedule updates\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy during training for the QAP model\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.plot(history_QAP.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_QAP.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss during training for the QAP model\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.plot(history_QAP.history['loss'], label='Train Loss')\n",
    "plt.plot(history_QAP.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When using QAP, convergence may be slightly slower compared to the full-precision model due to the combined effects of quantization and pruning.\n",
    "A stable validation curve indicates that efficiency gains are achieved without significant loss in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the test set using the QAP model\n",
    "y_pred_probs = modelQAP.predict(x_test_norm)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, convert it back to class indices\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Purples\")\n",
    "\n",
    "plt.title('Confusion Matrix for MNIST – QAP Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise:\n",
    "\n",
    "- Modify the bit-width (4, 8, 16, 32) and report the metrics for each model, considering both QAT and QAP. What conclusions can you draw?\n",
    "\n",
    "- Modify the MLP architecture by increasing or decreasing the number of layers, and replace the Flatten layer with a Dense layer.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation\n",
    "\n",
    "This technique focuses on transferring knowledge from a large network (teacher) to a smaller and faster target network (distilled or student). The student model learns to reproduce the behavior of the teacher architecture while being computationally more efficient.\n",
    "\n",
    "In knowledge distillation, the teacher model provides **soft labels**, which are probability distributions over classes rather than hard class labels. These soft labels contain richer information about class similarities and decision boundaries.\n",
    "\n",
    "A **temperature** parameter is introduced in the softmax function to control the smoothness of the output probabilities. Higher temperature values produce softer probability distributions, allowing the student model to better capture the teacher’s knowledge during training. During inference, the temperature is typically set back to 1.\n",
    "\n",
    "**Knowledge Distillation**: Hinton, G. (2015). _Distilling the Knowledge in a Neural Network_. arXiv preprint arXiv:1503.02531.\n",
    "\n",
    "![alt text](../img/lab02/distillation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Flatten images into 1D vectors\n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype=int)\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Teacher model (large MLP)\n",
    "\n",
    "def build_teacher():\n",
    "    model = keras.Sequential([\n",
    "        Dense(512, activation=\"relu\", input_shape=(28 * 28,)),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train the Teacher model\n",
    "teacher = build_teacher()\n",
    "\n",
    "# Display model architecture\n",
    "teacher.summary()\n",
    "\n",
    "teacher.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the teacher model\n",
    "history = teacher.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=16,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Student model (smaller MLP)\n",
    "\n",
    "def build_student():\n",
    "    model = keras.Sequential([\n",
    "        Dense(5, activation=\"relu\", input_shape=(28 * 28,)),\n",
    "        Dense(7, activation=\"relu\"),\n",
    "        Dense(10, activation=\"softmax\")  # Softmax output layer\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Instantiate the student model\n",
    "student = build_student()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels back to integer class indices (optional)\n",
    "train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Create the distillation wrapper (student + teacher)\n",
    "distilledMLP = Distiller(student=student, teacher=teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilledMLP.student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge distillation process\n",
    "\n",
    "adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the distillation model\n",
    "distilledMLP.compile(\n",
    "    optimizer=adam,\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    \n",
    "    # Loss for hard labels (ground truth)\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    \n",
    "    # Loss for soft labels (teacher predictions)\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    \n",
    "    # Weighting factor between hard-label loss and distillation loss\n",
    "    alpha=0.1,\n",
    "    \n",
    "    # Temperature used to soften the teacher and student outputs\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Train the student model using knowledge distillation\n",
    "history_distilledMLP = distilledMLP.fit(\n",
    "    x_train,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** A higher temperature produces softer probability distributions, allowing the student model to better capture the teacher’s knowledge.\n",
    "The parameter alpha controls the trade-off between learning from ground-truth labels and mimicking the teacher’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over epochs for the distilled student model\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(\n",
    "    history_distilledMLP.history['sparse_categorical_accuracy'],\n",
    "    label='Train Accuracy'\n",
    ")\n",
    "plt.plot(\n",
    "    history_distilledMLP.history['val_sparse_categorical_accuracy'],\n",
    "    label='Validation Accuracy'\n",
    ")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss over epochs for the distilled student model\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.plot(\n",
    "    history_distilledMLP.history['student_loss'],\n",
    "    label='Train Loss'\n",
    ")\n",
    "plt.plot(\n",
    "    history_distilledMLP.history['val_student_loss'],\n",
    "    label='Validation Loss'\n",
    ")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In knowledge distillation, the student model often achieves higher accuracy than a directly trained small model, as it benefits from the soft targets provided by the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the test set using the distilled student model\n",
    "y_pred_probs = distilledMLP.student.predict(x_test)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Since y_test is one-hot encoded, convert it back to class indices\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Purples')\n",
    "\n",
    "plt.title('Confusion Matrix for MNIST – Distilled Student Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Model\n",
    "\n",
    "Once the model has been trained and evaluated, it can be saved to disk for later use. Saving the model allows it to be reloaded without retraining, preserving both the learned weights and the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to disk\n",
    "distilledMLP.student.save(\"models/mnistKD.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "- Vary the **student model architecture** and analyze the performance of the distilled model using the appropriate evaluation metrics. Consider the following scenarios:\n",
    "  \n",
    "  - Decrease the number of layers.\n",
    "  - Vary the number of neurons in each layer.\n",
    "  - Increase the number of layers while reducing the number of neurons per layer.\n",
    "\n",
    "- Repeat the **model compression processes** for the **Fashion-MNIST** and **CIFAR-10** datasets.\n",
    "\n",
    "\n",
    "### Results Summary Table\n",
    "\n",
    "| Experiment | Dataset | Model Type | # Layers | Neurons per Layer | Compression Method | Bit-width | Final Sparsity | Train Accuracy | Validation Accuracy | Test Accuracy | Model Size / Params | Observations |\n",
    "|-----------|---------|------------|----------|-------------------|--------------------|-----------|----------------|----------------|---------------------|---------------|---------------------|--------------|\n",
    "| 1 | MNIST | Student (baseline) | | | None | FP32 | – | | | | | |\n",
    "| 2 | MNIST | Student | | | QAT | 8 | – | | | | | |\n",
    "| 3 | MNIST | Student | | | QAP | 8 | 0.3 | | | | | |\n",
    "| 4 | MNIST | Student | | | QAT + Distillation | 8 | – | | | | | |\n",
    "| 5 | Fashion-MNIST | Student | | | QAT | 8 | – | | | | | |\n",
    "| 6 | Fashion-MNIST | Student | | | QAP | 8 | 0.3 | | | | | |\n",
    "| 7 | CIFAR-10 | Student | | | QAT | 8 | – | | | | | |\n",
    "| 8 | CIFAR-10 | Student | | | QAT + Distillation | 8 | – | | | | | |\n",
    "\n",
    "\n",
    "**Guiding Questions:**\n",
    "- How does reducing the number of layers affect accuracy and generalization?\n",
    "- Is it more effective to reduce depth or width when compressing the model?\n",
    "- At what point does compression significantly degrade performance?\n",
    "- Does knowledge distillation help recover accuracy lost due to quantization or pruning?\n",
    "- Which configuration offers the best trade-off between accuracy and efficiency?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To continue with the next Machine Learning laboratory:\n",
    "\n",
    "  - **For the MNIST dataset, define a binary classifier** that discriminates only between digits **6 and 9**, or between **7 and 5**.  \n",
    "    Apply **Quantization-Aware Pruning (QAP)** as the training and compression method.\n",
    "\n",
    "  - **For the same binary classification scenario**, apply **Quantization-Aware Training (QAT)** combined with **Knowledge Distillation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
